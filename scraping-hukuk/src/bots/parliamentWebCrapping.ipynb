{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_keyword(driver, keyword):\n",
    "    try:\n",
    "        search_box = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located(\n",
    "            (By.ID, \"broi_form:_idJsp61\"))\n",
    "    )\n",
    "\n",
    "        print(\"element exists\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"element not found\")\n",
    "    \n",
    "\n",
    "    search_box.clear()\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    #print(f\"Current URL: {driver.current_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats the date string to YYYY-MM-DD format.\n",
    "    You may need to adjust this function based on the actual date format on the website.\n",
    "    \"\"\"\n",
    "    # This is a placeholder implementation. Adjust as needed.\n",
    "    from datetime import datetime\n",
    "    date_obj = datetime.strptime(date_string, \"%d.%m.%Y\")\n",
    "    return date_obj.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(keyword: str, metadata: dict):\n",
    "    metadata_folder = os.path.join(f'data/raw/parliament', keyword.replace(':', '').replace(' ', '_'), 'metadata')\n",
    "    os.makedirs(metadata_folder, exist_ok=True)\n",
    "    print(metadata_folder)\n",
    "    metadata_file_name = os.path.join(metadata_folder, f\"metadata_{metadata['notified_date']}-{metadata['name']}.json\")\n",
    "    with open(metadata_file_name, 'w', encoding='utf-8') as metadata_file:\n",
    "        json.dump(metadata, metadata_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary(keyword: str, url: str, date: str, name: str, description: str):\n",
    "        text_folder = os.path.join(f'data/raw/parliament', keyword.replace(':', '').replace(' ', '_'), 'text')\n",
    "        os.makedirs(text_folder, exist_ok=True)\n",
    "        summary_file_name = os.path.join(text_folder, f\"{date}-{name}.txt\")\n",
    "        with open(summary_file_name, 'w', encoding='utf-8') as summary_file:\n",
    "            summary_file.write(f\"Title: {name}\\n\")\n",
    "            summary_file.write(f\"Distribution date: {date}\\n\")\n",
    "            summary_file.write(f\"Keywords: {keyword}\\n\")\n",
    "            summary_file.write(f\"Summary: {description}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element exists\n",
      "Page has:1 item\n",
      "Document Name: Брой_71\n",
      "Table not found on the page.\n",
      "1\n",
      "data/raw/parliament\\Competition_Law\\metadata\n",
      "No more pages to scrape\n",
      "Renamed: C:\\Users\\10138283\\OneDrive - NTT DATA Business Solutions AG\\Desktop\\myprojects\\ds-sisecam-webscrapping\\data\\raw\\parliament\\Competition_Law\\pdf\\2018-08-28-Брой_71.pdf to C:\\Users\\10138283\\OneDrive - NTT DATA Business Solutions AG\\Desktop\\myprojects\\ds-sisecam-webscrapping\\data\\raw\\parliament\\Competition_Law\\pdf\\2018-08-28-Брой_71.pdf\n"
     ]
    }
   ],
   "source": [
    "# Give the keyword and download dir for pdf files\n",
    "mykeyword = \"Competition Law\"\n",
    "\n",
    "download_dir = \"C:\\\\Users\\\\10138283\\\\OneDrive - NTT DATA Business Solutions AG\\\\Desktop\\\\myprojects\\\\ds-sisecam-webscrapping\\\\data\\\\raw\\\\parliament\\\\\"+mykeyword.replace(' ', '_')+\"\\\\pdf\"\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {'download.default_directory' : download_dir}\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "base_url = \"https://dv.parliament.bg/DVWeb/broeveList.faces\"\n",
    "driver.get(base_url)\n",
    "\n",
    "def get_urls(driver, keyword: str, limited_page: int):\n",
    "    \n",
    "    time.sleep(2)\n",
    "    search_for_keyword(driver, keyword)\n",
    "    time.sleep(2)   \n",
    "    file_names = []\n",
    "\n",
    "    for page in range(2, limited_page + 1):\n",
    "        try:\n",
    "            no=0 # For accessing related item with its id, no incrementing +1 after every iteration\n",
    "\n",
    "            tbody = driver.find_element(By.ID, \"broi_form:dataTable1:tbody_element\")\n",
    "            td_elements = tbody.find_elements(By.CLASS_NAME, \"td_tabResult0\")\n",
    "\n",
    "            print(\"Page has:\" +str(len(td_elements))+ \" item\")\n",
    "\n",
    "            # Extract and print dates\n",
    "            for i in range(len(td_elements)):\n",
    "                url = \"\"\n",
    "                try:\n",
    "                    # Extract text content\n",
    "                    tbody = driver.find_element(By.ID, \"broi_form:dataTable1:tbody_element\")\n",
    "                    # Find all td elements with class \"td_tabResult0\"\n",
    "                    td_elements = tbody.find_elements(By.CLASS_NAME, \"td_tabResult0\")\n",
    "\n",
    "                    text = td_elements[i].text\n",
    "                    splitted_text = text.split(\",\")\n",
    "                    name = splitted_text[0].strip().replace('/', '_').replace(':', '').replace(' ', '_').replace('\\n', '_')\n",
    "                    print(\"Document Name: \"+name)\n",
    "\n",
    "                    # Extract Date\n",
    "                    date = re.findall(r'\\b\\d{1,2}\\.\\d{1,2}\\.\\d{4}\\b', splitted_text[1])[0]\n",
    "                    formatted_date = format_date(date)\n",
    "\n",
    "                    file_names.append(str(formatted_date)+\"-\"+name+\".pdf\")\n",
    "                    # Download button clik\n",
    "                    link = tbody.find_element(By.ID, f\"broi_form:dataTable1:{no}:_idJsp109\")\n",
    "                    link.click()\n",
    "                    time.sleep(2)\n",
    "\n",
    "                \n",
    "                    if driver.find_element(By.ID, \"broi_form:end_fixed_div\").is_displayed():\n",
    "                        print(\"tablo gözüktü\")\n",
    "                        table = driver.find_element(By.CLASS_NAME, \"border2.white\")\n",
    "                        links = table.find_elements(By.TAG_NAME, \"a\")\n",
    "                        first_link = links[0]\n",
    "                        url = first_link.get_attribute('href')\n",
    "                        first_link.click()\n",
    "                        time.sleep(2)\n",
    "\n",
    "                        close_button = table.find_element(By.XPATH, \"/html/body/div/form/div[2]/div/table/tbody/tr[1]/td[2]/img\")\n",
    "                        close_button.click()\n",
    "                        print(f\"Link text: {links[0].text}, Href: {url}\")\n",
    "\n",
    "                    else:\n",
    "                        url = link.get_attribute('href')\n",
    "                        print(f\"Link text: {link.text}, Href: {url}\")   \n",
    "\n",
    "                except NoSuchElementException:\n",
    "                    print(\"Table not found on the page.\")\n",
    "                no +=1\n",
    "                print(no)\n",
    "                if url == \"\":\n",
    "                    url = \"https://dv.parliament.bg/DVWeb/broeveList.faces#\"\n",
    "                \n",
    "                metadata_dict = {\n",
    "                \"name\": name,\n",
    "                \"notified_date\": formatted_date,\n",
    "                \"notified_country\": None,\n",
    "                \"URL\": url,\n",
    "                \"keyword\": mykeyword\n",
    "                }\n",
    "                save_metadata(keyword=mykeyword, metadata=metadata_dict)\n",
    "                save_summary(keyword=mykeyword, url=url, date=formatted_date, name=name, description=\"This is a pdf file.\")\n",
    "                time.sleep(2)\n",
    "            # Changing Page Part\n",
    "            page_element = driver.find_element(By.ID, \"broi_form:selectPageTop\")\n",
    "            select = Select(page_element)\n",
    "            select.select_by_value(str(page))\n",
    "            \n",
    "            print(f\"Scraping page {page}\")\n",
    "            time.sleep(4)\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            print(\"No more pages to scrape\")\n",
    "            break\n",
    "    downloaded_file = [os.path.join(download_dir, f) for f in os.listdir(download_dir)]\n",
    "    # Dosyaları oluşturulma zamanına göre sıralıyoruz\n",
    "    downloaded_file.sort(key=os.path.getctime)\n",
    "    time.sleep(2)\n",
    "    for old_file, new_name in zip(downloaded_file, file_names):\n",
    "        # Yeni dosya yolunu oluştur\n",
    "        new_file_path = os.path.join(download_dir, new_name)\n",
    "        \n",
    "        # Eski dosyanın ismini yenisiyle değiştir\n",
    "        os.rename(old_file, new_file_path)\n",
    "\n",
    "        print(f\"Renamed: {old_file} to {new_file_path}\")\n",
    "    driver.quit()\n",
    "\n",
    "get_urls(driver, mykeyword, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pegasus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
